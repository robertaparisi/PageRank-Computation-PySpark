{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/java\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/roberta/jdk1.8.0_211'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !export JAVA_HOME=~/jdk1.8.0_211\n",
    "# !export PATH=\"$JAVA_HOME/bin:$PATH\"\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/home/roberta/jdk1.8.0_211\"\n",
    "os.environ[\"JRE_HOME\"] = \"/home/roberta/jdk1.8.0_211/jre\"\n",
    "!which java\n",
    "os.getenv(\"JAVA_HOME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import pyspark as py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "import operator\n",
    "import pagerankPySpark as PRpy\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SpamPageRank\").config(\"spark.memory.fraction\", 0.8).config(\"spark.executor.memory\", \"14g\").config(\"spark.driver.memory\", \"128g\").config(\"spark.sql.shuffle.partitions\" , \"800\").config(\"spark.driver.maxResultSize\", \"12g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start counting the ougoing and incoming link for the entire graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_structure = StructType(fields= [StructField('networkID', StringType(), True), \n",
    "                                   StructField('clueWebID', StringType(), True)])\n",
    "filename = \"ClueB-ID-DOCNO.txt\"\n",
    "mapping = spark.read.csv(filename, sep = '\\t', schema=df_structure) #148148553 osservazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_structure = StructType(fields= [StructField('GoingFrom', StringType(), True), \n",
    "                                   StructField('PointingTo', StringType(), True)])\n",
    "filename = \"ClueWeb09_50m_Network.csv\"\n",
    "networkDF = spark.read.csv(filename, sep = ' ', schema=df_structure) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netw = networkDF.join(mapping, networkDF.GoingFrom == mapping.networkID).select(col('clueWebID').alias('GoingFrom'), 'PointingTo') \n",
    "netDF = netw.join(mapping, netw.PointingTo == mapping.networkID).select('GoingFrom',col('clueWebID').alias('PointingTo')) \n",
    "netDFpandas= netDF.toPandas()\n",
    "netDFpandas.to_csv('networkDF.csv', index=False, header = None, sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes = mapping.select('clueWebID').toPandas()\n",
    "# nodes.to_csv('nodes.csv', index=False, header = None)\n",
    "filename = 'networkDF.csv'\n",
    "nodesFilename = 'nodes.csv'\n",
    "number_of_nodes = 148148553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Network Analysis: after this computation we will have for each nodes the Number of Outgoing and Incoming Link and the pagerank score\n",
      "compute_pagerankFalse\n",
      "computing incoming link count, outgoing...\n",
      "Computing the number of Incoming Links\n",
      "Computing the number of Outgming Links\n"
     ]
    }
   ],
   "source": [
    "compute_pagerank = False\n",
    "outgoingLink, incomingLink= PRpy.networkDataComputation(spark, filename, nodesFilename, compute_pagerank, number_of_nodes)#37 iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomingLinkDF = incomingLink.toDF()\n",
    "incomingLink = incomingLinkDF.toPandas()\n",
    "incomingLink.columns = [\"ClueWebID\",\"IncomingLink\"]\n",
    "incomingLink.to_csv(\"completeIncomingLink.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del inncomingLink\n",
    "del inncomingLinkDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outgoingLinkDF = outgoingLink.toDF()\n",
    "outgoingLink = outgoingLinkDF.toPandas()\n",
    "outgoingLink.columns = [\"ClueWebID\",\"OutgoingLink\"]\n",
    "outgoingLink.to_csv(\"completeOutgoingLink.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the same measures for the filtered dataset (the page classified  as spam will not be part of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_structure = StructType(fields= [StructField('score', IntegerType(), True), \n",
    "                                   StructField('clueWebID', StringType(), True)])\n",
    "filename = \"clueweb09spam_Fusion.csv\"\n",
    "spamScore = spark.read.csv(filename, sep = ' ', schema=df_structure) # 503903810 osservazioni di cui 150955774 con uno spam score maggiore di 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamScore = spamScore.join(mapping, on = 'clueWebID')\n",
    "spamDF = spamScore.filter(spamScore.score>=70) # 61324401\n",
    "networkDF1 = networkDF.join(spamDF, networkDF.GoingFrom == spamDF.networkID).select(col('clueWebID').alias('GoingFrom'), 'PointingTo') \n",
    "#202015555 number of nodes in GoingFrom that are not in SPAM\n",
    "networkDFwithoutSPAM =  networkDF1.join(spamDF, networkDF1.PointingTo == spamDF.networkID).select('GoingFrom',col('clueWebID').alias('PointingTo')) \n",
    "#101776085 number of links in GoingFrom that are not in SPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkWithoutSPAM = networkDFwithoutSPAM.toPandas()\n",
    "networkWithoutSPAM.to_csv('networkWithoutSPAM.csv', index=False, header = None, sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.85\n",
    "max_iter = 100\n",
    "tol = 1e-6\n",
    "filename = 'networkWithoutSPAM.csv'\n",
    "filename = 'networkDF.csv'\n",
    "nodesFilename = 'nodes.csv'\n",
    "number_of_nodes = 148148553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_pagerank = False\n",
    "outgoingLink, incomingLink, score, iteration = PRpy.networkDataComputation(spark, filename, nodesFilename, alpha, max_iter, tol, number_of_nodes)#37 iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankPySpark = score.toDF()\n",
    "pandasDF = rankPySpark.toPandas()\n",
    "pandasDF.columns = [\"clueWebID\",\"PR_Score\"]\n",
    "pandasDF.to_csv(\"SpamPageRankScore.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomingLinkDF = incomingLink.toDF()\n",
    "incomingLink = incomingLinkDF.toPandas()\n",
    "incomingLink.columns = [\"ClueWebID\",\"IncomingLink\"]\n",
    "incomingLink.to_csv(\"spamFreeIncomingLink.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outgoingLinkDF = outgoingLink.toDF()\n",
    "outgoingLink = outgoingLinkDF.toPandas()\n",
    "outgoingLink.columns = [\"ClueWebID\",\"OutgoingLink\"]\n",
    "outgoingLink.to_csv(\"spamFreeOutgoingLink.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (roberta-env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
